{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.0. IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T14:51:00.142182Z",
     "start_time": "2020-07-30T14:51:00.130178Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy  as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import pickle\n",
    "import requests\n",
    "import warnings\n",
    "import inflection\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "import datetime\n",
    "\n",
    "from datetime              import datetime, timedelta\n",
    "from scipy                 import stats  as ss\n",
    "from boruta                import BorutaPy\n",
    "from matplotlib            import pyplot as plt\n",
    "from IPython.display       import Image\n",
    "from IPython.core.display  import HTML\n",
    "\n",
    "\n",
    "from sklearn.metrics       import mean_absolute_error, mean_squared_error\n",
    "from sklearn.ensemble      import RandomForestRegressor\n",
    "from sklearn.linear_model  import LinearRegression, Lasso\n",
    "from sklearn.preprocessing import RobustScaler, MinMaxScaler, LabelEncoder\n",
    "\n",
    "warnings.filterwarnings( 'ignore' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation( x_training, kfold, model_name, model, verbose=False ):\n",
    "    mae_list = []\n",
    "    mape_list = []\n",
    "    rmse_list = []\n",
    "    for k in reversed( range( 1, kfold+1 ) ):\n",
    "        if verbose:\n",
    "            print( '\\nKFold Number: {}'.format( k ) )\n",
    "        # start and end date for validation \n",
    "        validation_start_date = x_training['date'].max() - timedelta( days=k*6*7)\n",
    "        validation_end_date = x_training['date'].max() - timedelta( days=(k-1)*6*7)\n",
    "\n",
    "        # filtering dataset\n",
    "        training = x_training[x_training['date'] < validation_start_date]\n",
    "        validation = x_training[(x_training['date'] >= validation_start_date) & (x_training['date'] <= validation_end_date)]\n",
    "\n",
    "        # training and validation dataset\n",
    "        # training\n",
    "        xtraining = training.drop( ['date', 'sales'], axis=1 ) \n",
    "        ytraining = training['sales']\n",
    "\n",
    "        # validation\n",
    "        xvalidation = validation.drop( ['date', 'sales'], axis=1 )\n",
    "        yvalidation = validation['sales']\n",
    "\n",
    "        # model\n",
    "        m = model.fit( xtraining, ytraining )\n",
    "\n",
    "        # prediction\n",
    "        yhat = m.predict( xvalidation )\n",
    "\n",
    "        # performance\n",
    "        m_result = ml_error( model_name, np.expm1( yvalidation ), np.expm1( yhat ) )\n",
    "\n",
    "        # store performance of each kfold iteration\n",
    "        mae_list.append(  m_result['MAE'] )\n",
    "        mape_list.append( m_result['MAPE'] )\n",
    "        rmse_list.append( m_result['RMSE'] )\n",
    "\n",
    "    return pd.DataFrame( {'Model Name': model_name,\n",
    "                          'MAE CV': np.round( np.mean( mae_list ), 2 ).astype( str ) + ' +/- ' + np.round( np.std( mae_list ), 2 ).astype( str ),\n",
    "                          'MAPE CV': np.round( np.mean( mape_list ), 2 ).astype( str ) + ' +/- ' + np.round( np.std( mape_list ), 2 ).astype( str ),\n",
    "                          'RMSE CV': np.round( np.mean( rmse_list ), 2 ).astype( str ) + ' +/- ' + np.round( np.std( rmse_list ), 2 ).astype( str ) }, index=[0] )\n",
    "\n",
    "\n",
    "def mean_percentage_error( y, yhat ):\n",
    "    return np.mean( ( y - yhat ) / y )\n",
    "     \n",
    "    \n",
    "def mean_absolute_percentage_error( y, yhat ):\n",
    "    return np.mean( np.abs( ( y - yhat ) / y ) )\n",
    "\n",
    "    \n",
    "def ml_error( model_name, y, yhat ):\n",
    "    mae = mean_absolute_error( y, yhat )\n",
    "    mape = mean_absolute_percentage_error( y, yhat )\n",
    "    rmse = np.sqrt( mean_squared_error( y, yhat ) )\n",
    "    \n",
    "    return pd.DataFrame( { 'Model Name': model_name, \n",
    "                           'MAE': mae, \n",
    "                           'MAPE': mape,\n",
    "                           'RMSE': rmse }, index=[0] )\n",
    "\n",
    "def cramer_v( x, y ):\n",
    "    cm = pd.crosstab( x, y ).as_matrix()\n",
    "    n = cm.sum()\n",
    "    r, k = cm.shape\n",
    "    \n",
    "    chi2 = ss.chi2_contingency( cm )[0]\n",
    "    chi2corr = max( 0, chi2 - (k-1)*(r-1)/(n-1) )\n",
    "    \n",
    "    kcorr = k - (k-1)**2/(n-1)\n",
    "    rcorr = r - (r-1)**2/(n-1)\n",
    "    \n",
    "    return np.sqrt( (chi2corr/n) / ( min( kcorr-1, rcorr-1 ) ) )\n",
    "\n",
    "\n",
    "\n",
    "# def jupyter_settings():\n",
    "#     %matplotlib inline\n",
    "#     %pylab inline\n",
    "#     \n",
    "#     plt.style.use( 'bmh' )\n",
    "#     plt.rcParams['figure.figsize'] = [25, 12]\n",
    "#     plt.rcParams['font.size'] = 24\n",
    "#     \n",
    "#     display( HTML( '<style>.container { width:100% !important; }</style>') )\n",
    "#     pd.options.display.max_columns = None\n",
    "#     pd.options.display.max_rows = None\n",
    "#     pd.set_option( 'display.expand_frame_repr', False )\n",
    "#     \n",
    "#     sns.set()\n",
    "# jupyter_settings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2. Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales_raw = pd.read_csv( 'data/train.csv', low_memory=False )\n",
    "df_store_raw = pd.read_csv( 'data/store.csv', low_memory=False )\n",
    "\n",
    "# merge\n",
    "df_raw = pd.merge( df_sales_raw, df_store_raw, how='left', on='Store' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. DESCRIÇÃO DOS DADOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df_raw.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Rename Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_old = ['Store', 'DayOfWeek', 'Date', 'Sales', 'Customers', 'Open', 'Promo', 'StateHoliday', 'SchoolHoliday', \n",
    "            'StoreType', 'Assortment', 'CompetitionDistance', 'CompetitionOpenSinceMonth',\n",
    "            'CompetitionOpenSinceYear', 'Promo2', 'Promo2SinceWeek', 'Promo2SinceYear', 'PromoInterval']\n",
    "\n",
    "snakecase = lambda x: inflection.underscore( x )\n",
    "\n",
    "cols_new = list( map( snakecase, cols_old ) )\n",
    "\n",
    "# rename\n",
    "df1.columns = cols_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Data Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Number of Rows: 1017209\nNumber of Cols: 18\n"
    }
   ],
   "source": [
    "print( 'Number of Rows: {}'.format( df1.shape[0] ) )\n",
    "print( 'Number of Cols: {}'.format( df1.shape[1] ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "store                                    int64\nday_of_week                              int64\ndate                            datetime64[ns]\nsales                                    int64\ncustomers                                int64\nopen                                     int64\npromo                                    int64\nstate_holiday                           object\nschool_holiday                           int64\nstore_type                              object\nassortment                              object\ncompetition_distance                   float64\ncompetition_open_since_month           float64\ncompetition_open_since_year            float64\npromo2                                   int64\npromo2_since_week                      float64\npromo2_since_year                      float64\npromo_interval                          object\ndtype: object"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "df1['date'] = pd.to_datetime( df1['date'] )\n",
    "df1.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Fillout NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#competition_distance        \n",
    "df1['competition_distance'] = df1['competition_distance'].apply( lambda x: 200000.0 if math.isnan( x ) else x )\n",
    "\n",
    "#competition_open_since_month\n",
    "df1['competition_open_since_month'] = df1.apply( lambda x: x['date'].month if math.isnan( x['competition_open_since_month'] ) else x['competition_open_since_month'], axis=1 )\n",
    "\n",
    "#competition_open_since_year \n",
    "df1['competition_open_since_year'] = df1.apply( lambda x: x['date'].year if math.isnan( x['competition_open_since_year'] ) else x['competition_open_since_year'], axis=1 )\n",
    "\n",
    "#promo2_since_week           \n",
    "df1['promo2_since_week'] = df1.apply( lambda x: x['date'].week if math.isnan( x['promo2_since_week'] ) else x['promo2_since_week'], axis=1 )\n",
    "\n",
    "#promo2_since_year           \n",
    "df1['promo2_since_year'] = df1.apply( lambda x: x['date'].year if math.isnan( x['promo2_since_year'] ) else x['promo2_since_year'], axis=1 )\n",
    "\n",
    "#promo_interval              \n",
    "month_map = {1: 'Jan',  2: 'Fev',  3: 'Mar',  4: 'Apr',  5: 'May',  6: 'Jun',  7: 'Jul',  8: 'Aug',  9: 'Sep',  10: 'Oct', 11: 'Nov', 12: 'Dec'}\n",
    "\n",
    "df1['promo_interval'].fillna(0, inplace=True )\n",
    "\n",
    "df1['month_map'] = df1['date'].dt.month.map( month_map )\n",
    "\n",
    "df1['is_promo'] = df1[['promo_interval', 'month_map']].apply( lambda x: 0 if x['promo_interval'] == 0 else 1 if x['month_map'] in x['promo_interval'].split( ',' ) else 0, axis=1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6. Change Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# competiton\n",
    "df1['competition_open_since_month'] = df1['competition_open_since_month'].astype( int )\n",
    "df1['competition_open_since_year'] = df1['competition_open_since_year'].astype( int )\n",
    "    \n",
    "# promo2\n",
    "df1['promo2_since_week'] = df1['promo2_since_week'].astype( int )\n",
    "df1['promo2_since_year'] = df1['promo2_since_year'].astype( int )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. # Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# year\n",
    "df2['year'] = df2['date'].dt.year\n",
    "\n",
    "# month\n",
    "df2['month'] = df2['date'].dt.month\n",
    "\n",
    "# day\n",
    "df2['day'] = df2['date'].dt.day\n",
    "\n",
    "# week of year\n",
    "df2['week_of_year'] = df2['date'].dt.weekofyear\n",
    "\n",
    "# year week\n",
    "df2['year_week'] = df2['date'].dt.strftime( '%Y-%W' )\n",
    "\n",
    "# competition since\n",
    "df2['competition_since'] = df2.apply( lambda x: datetime( year=x['competition_open_since_year'], month=x['competition_open_since_month'],day=1 ), axis=1 )\n",
    "df2['competition_time_month'] = ( ( df2['date'] - df2['competition_since'] )/30 ).apply( lambda x: x.days ).astype( int )\n",
    "\n",
    "# promo since\n",
    "df2['promo_since'] = df2['promo2_since_year'].astype( str ) + '-' + df2['promo2_since_week'].astype( str )\n",
    "df2['promo_since'] = df2['promo_since'].apply( lambda x: datetime.strptime( x + '-1', '%Y-%W-%w' ) - timedelta( days=7 ) )\n",
    "df2['promo_time_week'] = ( ( df2['date'] - df2['promo_since'] )/7 ).apply( lambda x: x.days ).astype( int )\n",
    "\n",
    "# assortment\n",
    "df2['assortment'] = df2['assortment'].apply( lambda x: 'basic' if x == 'a' else 'extra' if x == 'b' else 'extended' )\n",
    "\n",
    "# state holiday\n",
    "df2['state_holiday'] = df2['state_holiday'].apply( lambda x: 'public_holiday' if x == 'a' else 'easter_holiday' if x == 'b' else 'christmas' if x == 'c' else 'regular_day' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. FILTRAGEM DE VARIÁVEIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3.1. Filtragem das Linhas\n",
    "df3 = df3[(df3['open'] != 0) & (df3['sales'] > 0)]\n",
    "\n",
    "## 3.2. Selecao das Colunas\n",
    "cols_drop = ['customers', 'open', 'promo_interval', 'month_map']\n",
    "df3 = df3.drop( cols_drop, axis=1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. DATA PREPARATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = df3.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "## rescaling\n",
    "rs = RobustScaler()\n",
    "mms = MinMaxScaler()\n",
    "\n",
    "# competition distance\n",
    "df5['competition_distance'] = rs.fit_transform( df5[['competition_distance']].values )\n",
    "pickle.dump( rs, open( 'parameter/competition_distance_scaler.pkl', 'wb') )\n",
    "\n",
    "# competition time month\n",
    "df5['competition_time_month'] = rs.fit_transform( df5[['competition_time_month']].values )\n",
    "pickle.dump( rs, open( 'parameter/competition_time_month_scaler.pkl', 'wb') )\n",
    "\n",
    "# promo time week\n",
    "df5['promo_time_week'] = mms.fit_transform( df5[['promo_time_week']].values )\n",
    "pickle.dump( rs, open( 'parameter/promo_time_week_scaler.pkl', 'wb') )\n",
    "\n",
    "# year\n",
    "df5['year'] = mms.fit_transform( df5[['year']].values )\n",
    "pickle.dump( mms, open( 'parameter/year_scaler.pkl', 'wb') )\n",
    "\n",
    "## encoding\n",
    "# state_holiday - One Hot Encoding\n",
    "df5 = pd.get_dummies( df5, prefix=['state_holiday'], columns=['state_holiday'] )\n",
    "\n",
    "# store_type - Label Encoding\n",
    "le = LabelEncoder()\n",
    "df5['store_type'] = le.fit_transform( df5['store_type'] )\n",
    "pickle.dump( le, open( 'parameter/store_type_scaler.pkl', 'wb') )\n",
    "\n",
    "# assortment - Ordinal Encoding\n",
    "assortment_dict = {'basic': 1,  'extra': 2, 'extended': 3}\n",
    "df5['assortment'] = df5['assortment'].map( assortment_dict )\n",
    "\n",
    "## Response Variable Transformation\n",
    "df5['sales'] = np.log1p( df5['sales'] )\n",
    "\n",
    "## Nature Transformation\n",
    "# day of week\n",
    "df5['day_of_week_sin'] = df5['day_of_week'].apply( lambda x: np.sin( x * ( 2. * np.pi/7 ) ) )\n",
    "df5['day_of_week_cos'] = df5['day_of_week'].apply( lambda x: np.cos( x * ( 2. * np.pi/7 ) ) )\n",
    "\n",
    "# month\n",
    "df5['month_sin'] = df5['month'].apply( lambda x: np.sin( x * ( 2. * np.pi/12 ) ) )\n",
    "df5['month_cos'] = df5['month'].apply( lambda x: np.cos( x * ( 2. * np.pi/12 ) ) )\n",
    "\n",
    "# day \n",
    "df5['day_sin'] = df5['day'].apply( lambda x: np.sin( x * ( 2. * np.pi/30 ) ) )\n",
    "df5['day_cos'] = df5['day'].apply( lambda x: np.cos( x * ( 2. * np.pi/30 ) ) )\n",
    "\n",
    "# week of year\n",
    "df5['week_of_year_sin'] = df5['week_of_year'].apply( lambda x: np.sin( x * ( 2. * np.pi/52 ) ) )\n",
    "df5['week_of_year_cos'] = df5['week_of_year'].apply( lambda x: np.cos( x * ( 2. * np.pi/52 ) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 5. FEATURE SELECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6 = df5.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Training Min Date: 2013-01-01 00:00:00\nTraining Max Date: 2015-06-18 00:00:00\n\nTest Min Date: 2015-06-19 00:00:00\nTest Max Date: 2015-07-31 00:00:00\n"
    }
   ],
   "source": [
    "# Split dataframe into training and test dataset\n",
    "cols_drop = ['week_of_year', 'day', 'month', 'day_of_week', 'promo_since', 'competition_since', 'year_week' ]\n",
    "df6 = df6.drop( cols_drop, axis=1 )\n",
    "\n",
    "# training dataset\n",
    "X_train = df6[df6['date'] < '2015-06-19']\n",
    "y_train = X_train['sales']\n",
    "\n",
    "# test dataset\n",
    "X_test = df6[df6['date'] >= '2015-06-19']\n",
    "y_test = X_test['sales']\n",
    "\n",
    "print( 'Training Min Date: {}'.format( X_train['date'].min() ) )\n",
    "print( 'Training Max Date: {}'.format( X_train['date'].max() ) )\n",
    "\n",
    "print( '\\nTest Min Date: {}'.format( X_test['date'].min() ) )\n",
    "print( 'Test Max Date: {}'.format( X_test['date'].max() ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Manual Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_selected_boruta = [\n",
    "    'store',\n",
    "    'promo',\n",
    "    'store_type',\n",
    "    'assortment',\n",
    "    'competition_distance',\n",
    "    'competition_open_since_month',\n",
    "    'competition_open_since_year',\n",
    "    'promo2',\n",
    "    'promo2_since_week',\n",
    "    'promo2_since_year',\n",
    "    'competition_time_month',\n",
    "    'promo_time_week',\n",
    "    'day_of_week_sin',\n",
    "    'day_of_week_cos',\n",
    "    'month_sin',\n",
    "    'month_cos',\n",
    "    'day_sin',\n",
    "    'day_cos',\n",
    "    'week_of_year_sin',\n",
    "    'week_of_year_cos']\n",
    "\n",
    "# columns to add\n",
    "feat_to_add = ['date', 'sales']\n",
    "\n",
    "cols_selected_boruta_full = cols_selected_boruta.copy()\n",
    "cols_selected_boruta_full.extend( feat_to_add )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. MACHINE LEARNING MODELLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = X_train[ cols_selected_boruta ]\n",
    "x_test = X_test[ cols_selected_boruta ]\n",
    "\n",
    "# Time Series Data Preparation\n",
    "x_training = X_train[ cols_selected_boruta_full ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1. XGBoost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[13:13:07] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \nParameters: { colsample_bytee } might not be used.\n\n  This may not be accurate due to some parameters are only used in language bindings but\n  passed down to XGBoost core.  Or some parameters are not used but slip through this\n  verification. Please open an issue if you find above cases.\n\n\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "          Model Name          MAE      MAPE         RMSE\n0  XGBoost Regressor  6683.606385  0.949503  7330.742164",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model Name</th>\n      <th>MAE</th>\n      <th>MAPE</th>\n      <th>RMSE</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>XGBoost Regressor</td>\n      <td>6683.606385</td>\n      <td>0.949503</td>\n      <td>7330.742164</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 51
    }
   ],
   "source": [
    "# model\n",
    "model_xgb = xgb.XGBRegressor( objective='reg:squarederror',\n",
    "                              n_estimators=100, \n",
    "                              eta=0.01, \n",
    "                              max_depth=10, \n",
    "                              subsample=0.7,\n",
    "                              colsample_bytee=0.9 ).fit( x_train, y_train )\n",
    "\n",
    "# prediction\n",
    "yhat_xgb = model_xgb.predict( x_test )\n",
    "\n",
    "# performance\n",
    "xgb_result = ml_error( 'XGBoost Regressor', np.expm1( y_test ), np.expm1( yhat_xgb ) )\n",
    "xgb_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.1. XGBoost Regressor - Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\nKFold Number: 5\n"
    },
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "type object 'datetime.datetime' has no attribute 'timedelta'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-96-a34bd7220d44>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mxgb_result_cv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_validation\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mx_training\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'XGBoost Regressor'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_xgb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mxgb_result_cv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-aac4dc3ca693>\u001b[0m in \u001b[0;36mcross_validation\u001b[1;34m(x_training, kfold, model_name, model, verbose)\u001b[0m\n\u001b[0;32m      7\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;34m'\\nKFold Number: {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mk\u001b[0m \u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;31m# start and end date for validation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mvalidation_start_date\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_training\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'date'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimedelta\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mdays\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0mvalidation_end_date\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_training\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'date'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimedelta\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mdays\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: type object 'datetime.datetime' has no attribute 'timedelta'"
     ]
    }
   ],
   "source": [
    "xgb_result_cv = cross_validation( x_training, 5, 'XGBoost Regressor', model_xgb, verbose=True )\n",
    "xgb_result_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. HYPERPARAMETER FINE TUNING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_tuned = {\n",
    "    'n_estimators': 3000,\n",
    "    'eta': 0.03,\n",
    "    'max_depth': 5,\n",
    "    'subsample': 0.7,\n",
    "    'colsample_bytree': 0.7,\n",
    "    'min_child_weight': 3 \n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[13:18:25] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \nParameters: { colsample_bytee } might not be used.\n\n  This may not be accurate due to some parameters are only used in language bindings but\n  passed down to XGBoost core.  Or some parameters are not used but slip through this\n  verification. Please open an issue if you find above cases.\n\n\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "          Model Name         MAE      MAPE         RMSE\n0  XGBoost Regressor  760.056209  0.114528  1088.442059",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model Name</th>\n      <th>MAE</th>\n      <th>MAPE</th>\n      <th>RMSE</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>XGBoost Regressor</td>\n      <td>760.056209</td>\n      <td>0.114528</td>\n      <td>1088.442059</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 53
    }
   ],
   "source": [
    "# model\n",
    "model_xgb_tuned = xgb.XGBRegressor( objective='reg:squarederror',\n",
    "                                    n_estimators=param_tuned['n_estimators'], \n",
    "                                    eta=param_tuned['eta'], \n",
    "                                    max_depth=param_tuned['max_depth'], \n",
    "                                    subsample=param_tuned['subsample'],\n",
    "                                    colsample_bytee=param_tuned['colsample_bytree'],\n",
    "                                    min_child_weight=param_tuned['min_child_weight'] ).fit( x_train, y_train )\n",
    "\n",
    "# prediction\n",
    "yhat_xgb_tuned = model_xgb_tuned.predict( x_test )\n",
    "\n",
    "# performance\n",
    "xgb_result_tuned = ml_error( 'XGBoost Regressor', np.expm1( y_test ), np.expm1( yhat_xgb_tuned ) )\n",
    "xgb_result_tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "-0.01954731652908415"
     },
     "metadata": {},
     "execution_count": 55
    }
   ],
   "source": [
    "mpe = mean_percentage_error( np.expm1( y_test ), np.expm1( yhat_xgb_tuned ) )\n",
    "mpe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. DEPLOY MODEL TO PRODUCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Trained Model\n",
    "pickle.dump( model_xgb_tuned, open( 'C:/Users/ricar/repos/Data-Science-Em-Producao/model/model_rossmann.pkl', 'wb' ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1. Rossmann Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import inflection\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import datetime\n",
    "\n",
    "class Rossmann( object ):\n",
    "    def __init__( self ):\n",
    "        self.home_path='C:/Users/ricar/repos/Data-Science-Em-Producao/'\n",
    "        self.competition_distance_scaler   = pickle.load( open( self.home_path + 'parameter/competition_distance_scaler.pkl', 'rb') )\n",
    "        self.competition_time_month_scaler = pickle.load( open( self.home_path + 'parameter/competition_time_month_scaler.pkl', 'rb') )\n",
    "        self.promo_time_week_scaler        = pickle.load( open( self.home_path + 'parameter/promo_time_week_scaler.pkl', 'rb') )\n",
    "        self.year_scaler                   = pickle.load( open( self.home_path + 'parameter/year_scaler.pkl', 'rb') )\n",
    "        self.store_type_scaler             = pickle.load( open( self.home_path + 'parameter/store_type_scaler.pkl', 'rb') )\n",
    "        \n",
    "        \n",
    "    def data_cleaning( self, df1 ): \n",
    "        \n",
    "        ## 1.1. Rename Columns\n",
    "        cols_old = ['Store', 'DayOfWeek', 'Date', 'Open', 'Promo', 'StateHoliday', 'SchoolHoliday', \n",
    "                    'StoreType', 'Assortment', 'CompetitionDistance', 'CompetitionOpenSinceMonth',\n",
    "                    'CompetitionOpenSinceYear', 'Promo2', 'Promo2SinceWeek', 'Promo2SinceYear', 'PromoInterval']\n",
    "\n",
    "        snakecase = lambda x: inflection.underscore( x )\n",
    "\n",
    "        cols_new = list( map( snakecase, cols_old ) )\n",
    "\n",
    "        # rename\n",
    "        df1.columns = cols_new\n",
    "\n",
    "        ## 1.3. Data Types\n",
    "        df1['date'] = pd.to_datetime( df1['date'] )\n",
    "\n",
    "        ## 1.5. Fillout NA\n",
    "        #competition_distance        \n",
    "        df1['competition_distance'] = df1['competition_distance'].apply( lambda x: 200000.0 if math.isnan( x ) else x )\n",
    "\n",
    "        #competition_open_since_month\n",
    "        df1['competition_open_since_month'] = df1.apply( lambda x: x['date'].month if math.isnan( x['competition_open_since_month'] ) else x['competition_open_since_month'], axis=1 )\n",
    "\n",
    "        #competition_open_since_year \n",
    "        df1['competition_open_since_year'] = df1.apply( lambda x: x['date'].year if math.isnan( x['competition_open_since_year'] ) else x['competition_open_since_year'], axis=1 )\n",
    "\n",
    "        #promo2_since_week           \n",
    "        df1['promo2_since_week'] = df1.apply( lambda x: x['date'].week if math.isnan( x['promo2_since_week'] ) else x['promo2_since_week'], axis=1 )\n",
    "\n",
    "        #promo2_since_year           \n",
    "        df1['promo2_since_year'] = df1.apply( lambda x: x['date'].year if math.isnan( x['promo2_since_year'] ) else x['promo2_since_year'], axis=1 )\n",
    "\n",
    "        #promo_interval              \n",
    "        month_map = {1: 'Jan',  2: 'Fev',  3: 'Mar',  4: 'Apr',  5: 'May',  6: 'Jun',  7: 'Jul',  8: 'Aug',  9: 'Sep',  10: 'Oct', 11: 'Nov', 12: 'Dec'}\n",
    "\n",
    "        df1['promo_interval'].fillna(0, inplace=True )\n",
    "\n",
    "        df1['month_map'] = df1['date'].dt.month.map( month_map )\n",
    "\n",
    "        df1['is_promo'] = df1[['promo_interval', 'month_map']].apply( lambda x: 0 if x['promo_interval'] == 0 else 1 if x['month_map'] in x['promo_interval'].split( ',' ) else 0, axis=1 )\n",
    "\n",
    "        ## 1.6. Change Data Types\n",
    "        # competiton\n",
    "        df1['competition_open_since_month'] = df1['competition_open_since_month'].astype( int )\n",
    "        df1['competition_open_since_year'] = df1['competition_open_since_year'].astype( int )\n",
    "\n",
    "        # promo2\n",
    "        df1['promo2_since_week'] = df1['promo2_since_week'].astype( int )\n",
    "        df1['promo2_since_year'] = df1['promo2_since_year'].astype( int )\n",
    "        \n",
    "        return df1 \n",
    "\n",
    "\n",
    "    def feature_engineering( self, df2 ):\n",
    "\n",
    "        # year\n",
    "        df2['year'] = df2['date'].dt.year\n",
    "\n",
    "        # month\n",
    "        df2['month'] = df2['date'].dt.month\n",
    "\n",
    "        # day\n",
    "        df2['day'] = df2['date'].dt.day\n",
    "\n",
    "        # week of year\n",
    "        df2['week_of_year'] = df2['date'].dt.weekofyear\n",
    "\n",
    "        # year week\n",
    "        df2['year_week'] = df2['date'].dt.strftime( '%Y-%W' )\n",
    "\n",
    "        # competition since\n",
    "        df2['competition_since'] = df2.apply( lambda x: datetime.datetime( year=x['competition_open_since_year'], month=x['competition_open_since_month'],day=1 ), axis=1 )\n",
    "        df2['competition_time_month'] = ( ( df2['date'] - df2['competition_since'] )/30 ).apply( lambda x: x.days ).astype( int )\n",
    "\n",
    "        # promo since\n",
    "        df2['promo_since'] = df2['promo2_since_year'].astype( str ) + '-' + df2['promo2_since_week'].astype( str )\n",
    "        df2['promo_since'] = df2['promo_since'].apply( lambda x: datetime.datetime.strptime( x + '-1', '%Y-%W-%w' ) - datetime.timedelta( days=7 ) )\n",
    "        df2['promo_time_week'] = ( ( df2['date'] - df2['promo_since'] )/7 ).apply( lambda x: x.days ).astype( int )\n",
    "\n",
    "        # assortment\n",
    "        df2['assortment'] = df2['assortment'].apply( lambda x: 'basic' if x == 'a' else 'extra' if x == 'b' else 'extended' )\n",
    "\n",
    "        # state holiday\n",
    "        df2['state_holiday'] = df2['state_holiday'].apply( lambda x: 'public_holiday' if x == 'a' else 'easter_holiday' if x == 'b' else 'christmas' if x == 'c' else 'regular_day' )\n",
    "\n",
    "        # 3.0. PASSO 03 - FILTRAGEM DE VARIÁVEIS\n",
    "        ## 3.1. Filtragem das Linhas\n",
    "        df2 = df2[df2['open'] != 0]\n",
    "\n",
    "        ## 3.2. Selecao das Colunas\n",
    "        cols_drop = ['open', 'promo_interval', 'month_map']\n",
    "        df2 = df2.drop( cols_drop, axis=1 )\n",
    "        \n",
    "        return df2\n",
    "\n",
    "\n",
    "    def data_preparation( self, df5 ):\n",
    "\n",
    "        ## 5.2. Rescaling \n",
    "        # competition distance\n",
    "        df5['competition_distance'] = self.competition_distance_scaler.fit_transform( df5[['competition_distance']].values )\n",
    "    \n",
    "        # competition time month\n",
    "        df5['competition_time_month'] = self.competition_time_month_scaler.fit_transform( df5[['competition_time_month']].values )\n",
    "\n",
    "        # promo time week\n",
    "        df5['promo_time_week'] = self.promo_time_week_scaler.fit_transform( df5[['promo_time_week']].values )\n",
    "        \n",
    "        # year\n",
    "        df5['year'] = self.year_scaler.fit_transform( df5[['year']].values )\n",
    "\n",
    "        ### 5.3.1. Encoding\n",
    "        # state_holiday - One Hot Encoding\n",
    "        df5 = pd.get_dummies( df5, prefix=['state_holiday'], columns=['state_holiday'] )\n",
    "\n",
    "        # store_type - Label Encoding\n",
    "        df5['store_type'] = self.store_type_scaler.fit_transform( df5['store_type'] )\n",
    "\n",
    "        # assortment - Ordinal Encoding\n",
    "        assortment_dict = {'basic': 1,  'extra': 2, 'extended': 3}\n",
    "        df5['assortment'] = df5['assortment'].map( assortment_dict )\n",
    "\n",
    "        \n",
    "        ### 5.3.3. Nature Transformation\n",
    "        # day of week\n",
    "        df5['day_of_week_sin'] = df5['day_of_week'].apply( lambda x: np.sin( x * ( 2. * np.pi/7 ) ) )\n",
    "        df5['day_of_week_cos'] = df5['day_of_week'].apply( lambda x: np.cos( x * ( 2. * np.pi/7 ) ) )\n",
    "\n",
    "        # month\n",
    "        df5['month_sin'] = df5['month'].apply( lambda x: np.sin( x * ( 2. * np.pi/12 ) ) )\n",
    "        df5['month_cos'] = df5['month'].apply( lambda x: np.cos( x * ( 2. * np.pi/12 ) ) )\n",
    "\n",
    "        # day \n",
    "        df5['day_sin'] = df5['day'].apply( lambda x: np.sin( x * ( 2. * np.pi/30 ) ) )\n",
    "        df5['day_cos'] = df5['day'].apply( lambda x: np.cos( x * ( 2. * np.pi/30 ) ) )\n",
    "\n",
    "        # week of year\n",
    "        df5['week_of_year_sin'] = df5['week_of_year'].apply( lambda x: np.sin( x * ( 2. * np.pi/52 ) ) )\n",
    "        df5['week_of_year_cos'] = df5['week_of_year'].apply( lambda x: np.cos( x * ( 2. * np.pi/52 ) ) )\n",
    "        \n",
    "        \n",
    "        cols_selected = [ 'store', 'promo', 'store_type', 'assortment', 'competition_distance', 'competition_open_since_month',\n",
    "            'competition_open_since_year', 'promo2', 'promo2_since_week', 'promo2_since_year', 'competition_time_month', 'promo_time_week',\n",
    "            'day_of_week_sin', 'day_of_week_cos', 'month_sin', 'month_cos', 'day_sin', 'day_cos', 'week_of_year_sin', 'week_of_year_cos']\n",
    "        \n",
    "        return df5[ cols_selected ]\n",
    "    \n",
    "    \n",
    "    def get_prediction( self, model, original_data, test_data ):\n",
    "        # prediction\n",
    "        pred = model.predict( test_data )\n",
    "        \n",
    "        # join pred into the original data\n",
    "        original_data['prediction'] = np.expm1( pred )\n",
    "        \n",
    "        return original_data.to_json( orient='records', date_format='iso' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2. API Handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from flask             import Flask, request, Response\n",
    "from rossmann.Rossmann import Rossmann\n",
    "\n",
    "# loading model\n",
    "model = pickle.load( open( 'C:/Users/ricar/repos/Data-Science-Em-Producao/model/model_rossmann.pkl', 'rb') )\n",
    "\n",
    "# initialize API\n",
    "app = Flask( __name__ )\n",
    "\n",
    "@app.route( '/rossmann/predict', methods=['POST'] )\n",
    "def rossmann_predict():\n",
    "    test_json = request.get_json()\n",
    "   \n",
    "    if test_json: # there is data\n",
    "        if isinstance( test_json, dict ): # unique example\n",
    "            test_raw = pd.DataFrame( test_json, index=[0] )\n",
    "            \n",
    "        else: # multiple example\n",
    "            test_raw = pd.DataFrame( test_json, columns=test_json[0].keys() )\n",
    "            \n",
    "        # Instantiate Rossmann class\n",
    "        pipeline = Rossmann()\n",
    "        \n",
    "        # data cleaning\n",
    "        df1 = pipeline.data_cleaning( test_raw )\n",
    "        \n",
    "        # feature engineering\n",
    "        df2 = pipeline.feature_engineering( df1 )\n",
    "        \n",
    "        # data preparation\n",
    "        df3 = pipeline.data_preparation( df2 )\n",
    "        \n",
    "        # prediction\n",
    "        df_response = pipeline.get_prediction( model, test_raw, df3 )\n",
    "        \n",
    "        return df_response\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        return Reponse( '{}', status=200, mimetype='application/json' )\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run( 'localhost' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2. API Tester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading test dataset\n",
    "df10 = pd.read_csv( 'C:/Users/ricar/repos/Data-Science-Em-Producao/data/test.csv' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge test dataset + store\n",
    "df_test = pd.merge( df10, df_store_raw, how='left', on='Store' )\n",
    "\n",
    "# choose store for prediction\n",
    "df_test = df_test[df_test['Store'].isin( [20, 23, 22, 21, 3] )]\n",
    "\n",
    "# remove closed days\n",
    "df_test = df_test[df_test['Open'] != 0]\n",
    "df_test = df_test[~df_test['Open'].isnull()]\n",
    "df_test = df_test.drop( 'Id', axis=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert Dataframe to json\n",
    "import json\n",
    "data = json.dumps( df_test.to_dict( orient='records' ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Status Code 200\n"
    }
   ],
   "source": [
    "# API Call\n",
    "#url = 'http://localhost:5000/rossmann/predict'\n",
    "url = 'https://rossman-predict-model-nvs.herokuapp.com/rossmann/predict'\n",
    "header = {'Content-type': 'application/json' } \n",
    "data = data\n",
    "\n",
    "r = requests.post( url, data=data, headers=header )\n",
    "print( 'Status Code {}'.format( r.status_code ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = pd.DataFrame( r.json(), columns=r.json()[0].keys() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "A Loja Nº 3 venderá R$ 195,698.19 nas próximas 6 semanas\nA Loja Nº 20 venderá R$ 238,177.11 nas próximas 6 semanas\nA Loja Nº 21 venderá R$ 247,310.57 nas próximas 6 semanas\nA Loja Nº 22 venderá R$ 218,625.72 nas próximas 6 semanas\nA Loja Nº 23 venderá R$ 245,889.01 nas próximas 6 semanas\n"
    }
   ],
   "source": [
    "d2 = d1[['store', 'prediction']].groupby( 'store' ).sum().reset_index()\n",
    "\n",
    "for i in range( len( d2 ) ):\n",
    "    print( 'A Loja Nº {} venderá R$ {:,.2f} nas próximas 6 semanas'.format( \n",
    "            d2.loc[i, 'store'], \n",
    "            d2.loc[i, 'prediction'] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}